{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T07:38:13.610012Z",
     "start_time": "2020-02-24T07:38:13.602536Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import Functions as d2l\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda')\n",
    "\n",
    "DATA_ROOT = \"./Datasets/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T07:46:40.938039Z",
     "start_time": "2020-02-24T07:38:14.220478Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = os.path.join(DATA_ROOT, 'aclImdb_v1.tar.gz')\n",
    "if not os.path.exists(os.path.join(DATA_ROOT, 'aclImdb')):\n",
    "    with tarfile.open(fname, 'r') as f:\n",
    "        f.extractall(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T08:07:20.108142Z",
     "start_time": "2020-02-24T08:05:00.743502Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:34<00:00, 361.36it/s]\n",
      "100%|██████████| 12500/12500 [00:34<00:00, 365.69it/s]\n",
      "100%|██████████| 12500/12500 [00:34<00:00, 363.62it/s]\n",
      "100%|██████████| 12500/12500 [00:34<00:00, 363.84it/s]\n"
     ]
    }
   ],
   "source": [
    "###读取数据\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_imdb(folder='train', data_root = './Datasets/aclImdb'):\n",
    "    data = []\n",
    "    for label in ['pos','neg']:\n",
    "        folder_name = os.path.join(data_root, folder, label)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n','').lower()\n",
    "                data.append([review, 1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "train_data = read_imdb('train')\n",
    "test_data = read_imdb('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T07:51:48.967253Z",
     "start_time": "2020-02-24T07:51:48.510970Z"
    }
   },
   "source": [
    "## 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T15:03:01.826049Z",
     "start_time": "2020-02-24T15:02:59.329305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('# words in vocab:', 46152)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 基于空格进行分词\n",
    "def get_tokenized_imdb(data):\n",
    "    # data: list of [string, label]\n",
    "    def tokenizer(text):\n",
    "        return [token.lower() for token in text.split(' ')]\n",
    "    return [tokenizer(review) for review,_ in data]\n",
    "\n",
    "\n",
    "### 基于分词结果创造词典\n",
    "def get_vocab_imdb(data):\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=5)\n",
    "\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "'# words in vocab:',len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T15:08:02.476580Z",
     "start_time": "2020-02-24T15:08:02.470280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8366"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi['pad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T15:10:30.404170Z",
     "start_time": "2020-02-24T15:10:30.394220Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_imdb(data, vocab):\n",
    "    max_len = 500  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "    \n",
    "    def pad(x):\n",
    "        return x[:max_len] if len(x) > max_len else x + [0] * (max_len - len(x)) \n",
    "    \n",
    "    tokenized_data = get_tokenized_imdb(data)  ## 基于空格进行分词\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:34:28.602663Z",
     "start_time": "2020-02-25T04:34:23.021885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([128, 500]) y torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches:', 196)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test_data, vocab))\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n",
    "\n",
    "for x,y in train_iter:\n",
    "    print('x',x.shape, 'y',y.shape)\n",
    "    break\n",
    "'#batches:',len(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:34:30.402421Z",
     "start_time": "2020-02-25T04:34:30.351349Z"
    }
   },
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.encoder = nn.LSTM(input_size = embed_size,\n",
    "                               hidden_size = num_hiddens,\n",
    "                               num_layers = num_layers,\n",
    "                               bidirectional=True)\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是(batch, 500词),LSTM需要将序列长度作为第一维，所以要进行转置。\n",
    "        embeddings = self.embedding(inputs.permute(1, 0)) # 输出形状为(500词, 批量大小, 词向量维度)\n",
    "        # rnn.LSTM只传入输入embeddings，因此只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "        # outputs形状是(词数, 批量大小, 2 * 隐藏单元个数)\n",
    "        outputs, _ = self.encoder(embeddings) # output, (h, c)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为\n",
    "        # (批量大小, 4 * 隐藏单元个数)。\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs\n",
    "    \n",
    "    \n",
    "embed_size = 100\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:34:41.405228Z",
     "start_time": "2020-02-25T04:34:39.583773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有21202个超出词表的词\n"
     ]
    }
   ],
   "source": [
    "glove_vocab = Vocab.GloVe(name='6B', dim =100, cache=os.path.join(DATA_ROOT, 'glove'))\n",
    "\n",
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0])\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"有{}个超出词表的词\".format(oov_count))\n",
    "    return embed\n",
    "\n",
    "net.embedding.weight.data.copy_(\n",
    "    load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.embedding.weight.requires_grad = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:34:41.421917Z",
     "start_time": "2020-02-25T04:34:41.406763Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = d2l.evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:53:36.696726Z",
     "start_time": "2020-02-25T04:34:45.745780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.6365, train acc 0.616, test acc 0.801, time 22.6 sec\n",
      "epoch 2, loss 0.1963, train acc 0.824, test acc 0.820, time 22.6 sec\n",
      "epoch 3, loss 0.1077, train acc 0.861, test acc 0.860, time 22.6 sec\n",
      "epoch 4, loss 0.0715, train acc 0.881, test acc 0.868, time 22.5 sec\n",
      "epoch 5, loss 0.0482, train acc 0.901, test acc 0.858, time 22.7 sec\n",
      "epoch 6, loss 0.0360, train acc 0.914, test acc 0.862, time 22.7 sec\n",
      "epoch 7, loss 0.0256, train acc 0.930, test acc 0.863, time 22.7 sec\n",
      "epoch 8, loss 0.0189, train acc 0.942, test acc 0.869, time 22.5 sec\n",
      "epoch 9, loss 0.0146, train acc 0.950, test acc 0.863, time 22.6 sec\n",
      "epoch 10, loss 0.0162, train acc 0.935, test acc 0.855, time 22.8 sec\n",
      "epoch 11, loss 0.0133, train acc 0.943, test acc 0.856, time 22.6 sec\n",
      "epoch 12, loss 0.0097, train acc 0.956, test acc 0.855, time 22.6 sec\n",
      "epoch 13, loss 0.0073, train acc 0.966, test acc 0.853, time 22.6 sec\n",
      "epoch 14, loss 0.0062, train acc 0.969, test acc 0.852, time 22.5 sec\n",
      "epoch 15, loss 0.0054, train acc 0.969, test acc 0.855, time 23.4 sec\n",
      "epoch 16, loss 0.0044, train acc 0.973, test acc 0.853, time 22.6 sec\n",
      "epoch 17, loss 0.0037, train acc 0.977, test acc 0.848, time 22.6 sec\n",
      "epoch 18, loss 0.0032, train acc 0.979, test acc 0.853, time 22.7 sec\n",
      "epoch 19, loss 0.0027, train acc 0.982, test acc 0.852, time 22.5 sec\n",
      "epoch 20, loss 0.0028, train acc 0.980, test acc 0.852, time 22.6 sec\n",
      "epoch 21, loss 0.0021, train acc 0.984, test acc 0.846, time 22.6 sec\n",
      "epoch 22, loss 0.0024, train acc 0.981, test acc 0.851, time 22.7 sec\n",
      "epoch 23, loss 0.0019, train acc 0.984, test acc 0.847, time 22.6 sec\n",
      "epoch 24, loss 0.0018, train acc 0.984, test acc 0.848, time 22.6 sec\n",
      "epoch 25, loss 0.0013, train acc 0.989, test acc 0.849, time 22.7 sec\n",
      "epoch 26, loss 0.0013, train acc 0.987, test acc 0.846, time 22.6 sec\n",
      "epoch 27, loss 0.0012, train acc 0.988, test acc 0.848, time 22.7 sec\n",
      "epoch 28, loss 0.0015, train acc 0.985, test acc 0.846, time 22.6 sec\n",
      "epoch 29, loss 0.0013, train acc 0.986, test acc 0.849, time 22.5 sec\n",
      "epoch 30, loss 0.0012, train acc 0.987, test acc 0.848, time 22.6 sec\n",
      "epoch 31, loss 0.0014, train acc 0.984, test acc 0.850, time 22.6 sec\n",
      "epoch 32, loss 0.0010, train acc 0.990, test acc 0.844, time 22.6 sec\n",
      "epoch 33, loss 0.0010, train acc 0.989, test acc 0.847, time 22.6 sec\n",
      "epoch 34, loss 0.0009, train acc 0.989, test acc 0.842, time 22.7 sec\n",
      "epoch 35, loss 0.0007, train acc 0.993, test acc 0.844, time 22.5 sec\n",
      "epoch 36, loss 0.0011, train acc 0.986, test acc 0.848, time 22.6 sec\n",
      "epoch 37, loss 0.0009, train acc 0.989, test acc 0.846, time 22.5 sec\n",
      "epoch 38, loss 0.0008, train acc 0.989, test acc 0.851, time 22.5 sec\n",
      "epoch 39, loss 0.0007, train acc 0.991, test acc 0.848, time 22.6 sec\n",
      "epoch 40, loss 0.0008, train acc 0.990, test acc 0.850, time 22.6 sec\n",
      "epoch 41, loss 0.0007, train acc 0.991, test acc 0.844, time 22.6 sec\n",
      "epoch 42, loss 0.0008, train acc 0.988, test acc 0.845, time 22.5 sec\n",
      "epoch 43, loss 0.0007, train acc 0.991, test acc 0.842, time 22.6 sec\n",
      "epoch 44, loss 0.0008, train acc 0.988, test acc 0.846, time 22.6 sec\n",
      "epoch 45, loss 0.0006, train acc 0.991, test acc 0.849, time 22.6 sec\n",
      "epoch 46, loss 0.0007, train acc 0.991, test acc 0.846, time 22.5 sec\n",
      "epoch 47, loss 0.0005, train acc 0.992, test acc 0.853, time 22.6 sec\n",
      "epoch 48, loss 0.0004, train acc 0.993, test acc 0.854, time 22.6 sec\n",
      "epoch 49, loss 0.0006, train acc 0.992, test acc 0.845, time 22.6 sec\n",
      "epoch 50, loss 0.0007, train acc 0.989, test acc 0.847, time 22.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 50\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:53:42.559753Z",
     "start_time": "2020-02-25T04:53:42.552300Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    \"\"\"sentence是词语的列表\"\"\"\n",
    "    device = list(net.parameters())[0].device\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    return 'positive' if label.item() == 1 else 'negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:54:16.945359Z",
     "start_time": "2020-02-25T04:54:16.937070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'not', 'good']) ## the result is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wangkai3.6",
   "language": "python",
   "name": "wangkai3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
