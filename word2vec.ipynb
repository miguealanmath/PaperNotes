{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:12:34.045962Z",
     "start_time": "2020-02-23T06:12:32.816876Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import Functions as d2l\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:12:35.009347Z",
     "start_time": "2020-02-23T06:12:34.662763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42068"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert 'ptb.train.txt' in os.listdir('./datasets/ptb')\n",
    "\n",
    "with open('./datasets/ptb/ptb.train.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    raw_dataset = [st.split() for st in lines]\n",
    "\n",
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:12:35.642469Z",
     "start_time": "2020-02-23T06:12:35.632452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']\n",
      "# tokens: 15 ['pierre', '<unk>', 'N', 'years', 'old']\n",
      "# tokens: 11 ['mr.', '<unk>', 'is', 'chairman', 'of']\n"
     ]
    }
   ],
   "source": [
    "for st in raw_dataset[:3]:\n",
    "    print('# tokens:', len(st), st[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:12:38.059295Z",
     "start_time": "2020-02-23T06:12:37.753472Z"
    }
   },
   "outputs": [],
   "source": [
    "##只保留至少出现5次的词。\n",
    "counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "counter = dict(filter(lambda x:x[1] >= 5, counter.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:12:44.515273Z",
     "start_time": "2020-02-23T06:12:44.195111Z"
    }
   },
   "outputs": [],
   "source": [
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk : idx for idx,tk in enumerate(idx_to_token) }\n",
    "### 这是下面一段代码实现的功能\n",
    "# dataset = []\n",
    "# for st in raw_dataset:\n",
    "#     inside_list = []\n",
    "#     for tk in st:\n",
    "#         if tk in token_to_idx:\n",
    "#             inside_list.append(token_to_idx[tk])\n",
    "#     dataset.append(inside_list)\n",
    "\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in raw_dataset]\n",
    "num_tokens = sum([len(st) for st in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:12:47.888562Z",
     "start_time": "2020-02-23T06:12:46.129597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 375722'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###二次采样\n",
    "\n",
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "## if not discard 就保留在dataset中\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "'# tokens: %d' % sum([len(st) for st in subsampled_dataset]) # '# tokens: 375875'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:12:48.716682Z",
     "start_time": "2020-02-23T06:12:48.565090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# the: before=50770, after=2210\n",
      "# join: before=45, after=45\n"
     ]
    }
   ],
   "source": [
    "### 比较下采样前后不同频率的词的差别\n",
    "def compare_counts(token):\n",
    "    return '# {}: before={}, after={}'.format(token, \n",
    "    sum([st.count(token_to_idx[token]) for st in dataset]),\n",
    "    sum([st.count(token_to_idx[token]) for st in subsampled_dataset])\n",
    "    )\n",
    "\n",
    "print(compare_counts('the'))\n",
    "print(compare_counts('join'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:12:52.405124Z",
     "start_time": "2020-02-23T06:12:52.389856Z"
    }
   },
   "outputs": [],
   "source": [
    "### 提取中心词和背景词\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [],[]\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:   # 每个句子至少要有2个词才可能组成一对“中心词-背景词”\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                min(len(st), center_i + 1 + window_size)))\n",
    "            indices.remove(center_i)   ### 将中心词排除在背景词之外\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:12:53.393416Z",
     "start_time": "2020-02-23T06:12:53.384762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center: 0, has contexts: [1, 2]\n",
      "center: 1, has contexts: [0, 2, 3]\n",
      "center: 2, has contexts: [1, 3]\n",
      "center: 3, has contexts: [1, 2, 4, 5]\n",
      "center: 4, has contexts: [3, 5]\n",
      "center: 5, has contexts: [3, 4, 6]\n",
      "center: 6, has contexts: [4, 5]\n",
      "center: 7, has contexts: [8, 9]\n",
      "center: 8, has contexts: [7, 9]\n",
      "center: 9, has contexts: [8]\n"
     ]
    }
   ],
   "source": [
    "##创建一个人工数据集进行测试\n",
    "tiny_dataset = [list(range(7)), list(range(7,10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center: {}, has contexts: {}'.format(center,context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:16:22.133107Z",
     "start_time": "2020-02-23T06:16:18.653484Z"
    }
   },
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:13:30.849360Z",
     "start_time": "2020-02-23T06:13:03.082431Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                # 根据每次的权重(sampling_weights)随机生成k个次的索引作为噪音词\n",
    "                # 为了高效计算，可以将k设得稍大一些。\n",
    "                i, neg_candidates = 0, random.choices(population, sampling_weights, k = int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            # 噪声词不能是背景词\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]  \n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T04:40:20.514216Z",
     "start_time": "2020-02-23T04:40:20.327452Z"
    }
   },
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T07:12:45.383774Z",
     "start_time": "2020-02-23T07:12:45.375368Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "    \n",
    "    \n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T07:16:41.015950Z",
     "start_time": "2020-02-23T07:16:41.010000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []+[5,4]+[6]\n",
    "a.extend([4,45,4])\n",
    "[] + [[1,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T07:17:16.964837Z",
     "start_time": "2020-02-23T07:17:16.954026Z"
    }
   },
   "outputs": [],
   "source": [
    "## 小批量读取函数\n",
    "def batchify(data):\n",
    "    \"\"\"用作DataLoader的参数collate_fn: 输入是个长为batchsize的list, \n",
    "    list中的每个元素都是Dataset类调用__getitem__得到的结果\n",
    "    \"\"\"\n",
    "    max_len = max(len(context) + len(negative) for _, context, negative in data)  #防止每个样本之间的背景词与噪声词个数不同\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers.append(center)\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        \n",
    "        # 当contexts_negatives变量中的某个元素为填充项时，相同位置的掩码变量masks中的元素取0，否则取1。\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)] \n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T07:29:53.947261Z",
     "start_time": "2020-02-23T07:29:53.481583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conters shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 4\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                            collate_fn=batchify, num_workers = num_workers)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['conters', 'contexts_negatives', 'masks', 'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T09:03:01.485370Z",
     "start_time": "2020-02-23T09:03:01.477549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7230,  0.3931, -0.6425,  0.2207],\n",
       "        [ 0.1613, -0.3123,  1.3286,  0.2694],\n",
       "        [ 0.9413, -0.8451, -0.4251,  2.0284],\n",
       "        [-0.1609,  0.0389, -0.0814, -1.4092],\n",
       "        [ 0.5249, -0.6877,  1.2749, -0.0268],\n",
       "        [-0.7252,  1.6458,  2.9459,  0.2996],\n",
       "        [-0.2320,  0.0997, -1.0951, -1.5036],\n",
       "        [-0.3462, -0.0782,  1.2879,  0.3700],\n",
       "        [ 0.3870, -1.5230, -2.1753, -0.5353],\n",
       "        [ 0.1551,  2.1947, -0.1158, -0.2787],\n",
       "        [-1.1749, -0.7988, -2.6129,  0.0148],\n",
       "        [-1.7559, -0.2019,  0.6404, -0.5579],\n",
       "        [ 0.1842,  0.2501,  1.3361,  1.3462],\n",
       "        [-0.7281,  1.1711,  0.0310,  1.0496],\n",
       "        [ 0.3765,  1.2168, -0.5250, -0.0271],\n",
       "        [-0.4480,  1.0740,  0.2213, -1.5208],\n",
       "        [ 1.7914, -0.2365, -0.0797, -1.3140],\n",
       "        [ 1.6254, -1.0398, -0.7819, -0.2055],\n",
       "        [-0.9918,  0.5988,  1.1078,  0.4426],\n",
       "        [ 2.9781,  0.7062, -2.2642, -1.4162]], requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(num_embeddings=20, embedding_dim=4)\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T09:18:59.181619Z",
     "start_time": "2020-02-23T09:18:59.167904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1613, -0.3123,  1.3286,  0.2694],\n",
       "         [ 0.9413, -0.8451, -0.4251,  2.0284],\n",
       "         [-0.1609,  0.0389, -0.0814, -1.4092]],\n",
       "\n",
       "        [[ 0.5249, -0.6877,  1.2749, -0.0268],\n",
       "         [-0.7252,  1.6458,  2.9459,  0.2996],\n",
       "         [-0.2320,  0.0997, -1.0951, -1.5036]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]], dtype=torch.long)\n",
    "embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T09:22:43.701997Z",
     "start_time": "2020-02-23T09:22:43.675794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 6])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((2,1,4))\n",
    "Y = torch.ones((2,4,6))\n",
    "torch.bmm(X,Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T12:46:22.850614Z",
     "start_time": "2020-02-23T12:46:22.841344Z"
    }
   },
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)       # batch_size * 1 * embedding_dim\n",
    "    u = embed_u(contexts_and_negatives)       # batch_size * max_len * embedding_dim\n",
    "    pred = torch.bmm(v, u.permute(0,2,1))     # \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T03:53:40.930627Z",
     "start_time": "2020-02-24T03:53:40.921914Z"
    }
   },
   "outputs": [],
   "source": [
    "### 二元交叉熵损失函数\n",
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        # inputs.shape : (batch_size, len)\n",
    "        # targets.shape: as the shape as input\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        result = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none', weight=mask)\n",
    "        return result.mean(dim=1)\n",
    "loss = SigmoidBinaryCrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T03:54:33.677363Z",
     "start_time": "2020-02-24T03:54:33.644556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8740, 1.2100])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
    "# 标签变量label中的1和0分别代表背景词和噪声词\n",
    "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]])\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])  # 掩码变量\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.float().sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T04:05:38.681329Z",
     "start_time": "2020-02-24T04:05:38.630859Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim = embed_size),\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim = embed_size)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T04:24:17.705680Z",
     "start_time": "2020-02-24T04:24:17.688952Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda:3')\n",
    "    print(device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start , ls_sum, n = time.time(), 0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative,mask, label = [d.to(device) for d in batch]\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            \n",
    "            # 使用掩码变量mask来避免填充项对损失函数计算的影响\n",
    "            ls = (loss(pred.view(label.shape), label, mask) * mask.shape[1] / mask.float().sum(dim=1)).mean()\n",
    "            optimizer.zero_grad()\n",
    "            ls.backward()\n",
    "            optimizer.step()\n",
    "            ls_sum += ls.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch {}, loss {:.3f}, time {:.3f}s'.format(epoch + 1, ls_sum/n, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T04:25:57.693468Z",
     "start_time": "2020-02-24T04:24:28.350527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n",
      "epoch 1, loss 1.991, time 7.748s\n",
      "epoch 2, loss 0.654, time 7.189s\n",
      "epoch 3, loss 0.478, time 8.164s\n",
      "epoch 4, loss 0.423, time 8.465s\n",
      "epoch 5, loss 0.397, time 8.162s\n",
      "epoch 6, loss 0.381, time 8.215s\n",
      "epoch 7, loss 0.368, time 8.098s\n",
      "epoch 8, loss 0.359, time 8.617s\n",
      "epoch 9, loss 0.350, time 8.442s\n",
      "epoch 10, loss 0.342, time 8.560s\n"
     ]
    }
   ],
   "source": [
    "train(net, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T04:31:22.747229Z",
     "start_time": "2020-02-24T04:31:22.734532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.482:chips\n",
      "cosine sim=0.428:ibm\n",
      "cosine sim=0.428:memory\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    w = embed.weight.data\n",
    "    x = w[token_to_idx[query_token]]\n",
    "    cos = torch.matmul(w,x) / (torch.sum(w * w, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k = k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:\n",
    "        print('cosine sim={:.3f}:{}'.format(cos[i], (idx_to_token[i])))\n",
    "        \n",
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML3.6",
   "language": "python",
   "name": "ml3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
